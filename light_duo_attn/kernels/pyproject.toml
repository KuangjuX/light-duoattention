[build-system]
requires = ["setuptools"]
build-backend = "setuptools.build_meta"

[project]
name = "light-duo-attn-kernels"
version = "0.1.0"
description = "Light Duo Attention Kernels"
readme = "README.md"
requires-python = ">=3.12"
license = {text = "Apache License 2.0"}

dependencies = [
    "nvidia-cutlass-dsl==4.1.0",
    "torch",
    "einops",
]

[project.optional-dependencies]
dev = [
    "pytest",
    "ruff",
]

[project.urls]
Homepage = "https://github.com/Dao-AILab/flash-attention"
Repository = "https://github.com/Dao-AILab/flash-attention"

[tool.setuptools]
packages = ["sglang.srt.sparse_attention.kernels.attention."]
package-dir = {"sglang.srt.sparse_attention.kernels.attention." = "."}

[tool.ruff]
line-length = 100

[tool.ruff.lint]
ignore = [
    "E731",  # do not assign a lambda expression, use a def
    "E741",  # Do not use variables named 'I', 'O', or 'l'
    "F841",  # local variable is assigned to but never used
]
